    1: import torch
    1: import torch.nn as nn
    1: import torch.optim as optim
    1: from torch.utils.data import DataLoader
    1: from datasets import load_dataset
    1: from transformers import BertTokenizer
    1: import math
    1: from memory_profiler import profile
       # Load the IMDB dataset
    1: dataset = load_dataset("imdb")
    1: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
       
       # Function to tokenize and encode a batch of texts
    1: def tokenize_function(examples):
           return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=128)
       
       # Apply tokenization to the dataset
    1: train_dataset = dataset['train'].select(range(10000)).map(tokenize_function, batched=True)
    1: test_dataset = dataset['test'].select(range(10000)).map(tokenize_function, batched=True)
       
    1: train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
    1: test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
       
    1: train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    1: test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
       
       # Define the model with AuxiliaryLinear, ModifiedEncoder, ModifiedDecoder (as previously set up)
       
    2: class AuxiliaryLinear(nn.Module):
    1:     def __init__(self, in_features, out_features):
   15:         super(AuxiliaryLinear, self).__init__()
   15:         self.linear = nn.Linear(in_features, out_features, bias=False)
   30:         with torch.no_grad():
   15:             if in_features == out_features:
   15:                 self.linear.weight.copy_(torch.eye(in_features))
                   else:
                       min_dim = min(in_features, out_features)
                       self.linear.weight[:, :min_dim] = torch.eye(min_dim)
    1:     def forward(self, x):
               return self.linear(x)
       
       # [Define ModifiedEncoderLayer, ModifiedDecoderLayer, PositionalEncoding, ModifiedTransformerModel here]
    2: class ModifiedEncoderLayer(nn.Module):
    1:     def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
    3:         super(ModifiedEncoderLayer, self).__init__()
    3:         self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=drop_prob)
    3:         self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
    3:         self.dropout1 = nn.Dropout(drop_prob)
               
    6:         self.ffn = nn.Sequential(
    3:             nn.Linear(d_model, ffn_hidden),
    3:             nn.ReLU(),
    3:             nn.Dropout(drop_prob),
    3:             nn.Linear(ffn_hidden, d_model),
    3:             nn.Dropout(drop_prob)
               )
    3:         self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
       
               # Auxiliary operators
    3:         self.aux_self_attention = AuxiliaryLinear(d_model, d_model)
    3:         self.aux_ffn = AuxiliaryLinear(d_model, d_model)
           
    1:     def forward(self, src, src_mask=None):
               attn_output, _ = self.self_attention(src, src, src, attn_mask=src_mask)
               attn_output_fused = attn_output + self.aux_self_attention(src)
               src = self.norm1(src + self.dropout1(attn_output_fused))
               
               ffn_output = self.ffn(src)
               ffn_output_fused = ffn_output + self.aux_ffn(src)
               src = self.norm2(src + ffn_output_fused)
               return src
       
       # Modified Decoder Layer with Auxiliary Operators
    2: class ModifiedDecoderLayer(nn.Module):
    1:     def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
    3:         super(ModifiedDecoderLayer, self).__init__()
    3:         self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=drop_prob)
    3:         self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
    3:         self.dropout1 = nn.Dropout(drop_prob)
               
    3:         self.cross_attention = nn.MultiheadAttention(d_model, num_heads, dropout=drop_prob)
    3:         self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
    3:         self.dropout2 = nn.Dropout(drop_prob)
               
    6:         self.ffn = nn.Sequential(
    3:             nn.Linear(d_model, ffn_hidden),
    3:             nn.ReLU(),
    3:             nn.Dropout(drop_prob),
    3:             nn.Linear(ffn_hidden, d_model),
    3:             nn.Dropout(drop_prob)
               )
    3:         self.norm3 = nn.LayerNorm(d_model, eps=1e-6)
       
               # Auxiliary operators
    3:         self.aux_self_attention = AuxiliaryLinear(d_model, d_model)
    3:         self.aux_cross_attention = AuxiliaryLinear(d_model, d_model)
    3:         self.aux_ffn = AuxiliaryLinear(d_model, d_model)
           
    1:     def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
               attn_output, _ = self.self_attention(tgt, tgt, tgt, attn_mask=tgt_mask)
               attn_output_fused = attn_output + self.aux_self_attention(tgt)
               tgt = self.norm1(tgt + self.dropout1(attn_output_fused))
               
               cross_attn_output, _ = self.cross_attention(tgt, memory, memory, attn_mask=memory_mask)
               cross_attn_output_fused = cross_attn_output + self.aux_cross_attention(memory)
               tgt = self.norm2(tgt + self.dropout2(cross_attn_output_fused))
               
               ffn_output = self.ffn(tgt)
               ffn_output_fused = ffn_output + self.aux_ffn(tgt)
               tgt = self.norm3(tgt + ffn_output_fused)
               return tgt
       
       # Positional Encoding
    2: class PositionalEncoding(nn.Module):
    1:     def __init__(self, d_model, max_len=5000):
    1:         super(PositionalEncoding, self).__init__()
    1:         self.encoding = torch.zeros(max_len, d_model)
    1:         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    1:         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    1:         self.encoding[:, 0::2] = torch.sin(position * div_term)
    1:         self.encoding[:, 1::2] = torch.cos(position * div_term)
    1:         self.encoding = self.encoding.unsqueeze(0)
       
    1:     def forward(self, x):
               return x + self.encoding[:, :x.size(1), :]
    2: class ModifiedEncoder(nn.Module):
    1:     def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):
    1:         super(ModifiedEncoder, self).__init__()
    9:         self.layers = nn.ModuleList([
    3:             ModifiedEncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)  # Only 5 arguments
    4:             for _ in range(num_layers)
               ])
    1:         self.norm = nn.LayerNorm(d_model)
           
    1:     def forward(self, src, mask=None):
               for layer in self.layers:
                   src = layer(src, mask)
               return self.norm(src)
       
       
    2: class ModifiedDecoder(nn.Module):
    1:     def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):
    1:         super(ModifiedDecoder, self).__init__()
    9:         self.layers = nn.ModuleList([
    3:             ModifiedDecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)  # Only 5 arguments
    4:             for _ in range(num_layers)
               ])
    1:         self.norm = nn.LayerNorm(d_model)
           
    1:     def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
               for layer in self.layers:
                   tgt = layer(tgt, memory, tgt_mask, memory_mask)
               return self.norm(tgt)
       
       # Modified Transformer Model
    2: class ModifiedTransformerModel(nn.Module):
    1:     def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, vocab_size, num_classes):
    1:         super(ModifiedTransformerModel, self).__init__()
    1:         self.embedding = nn.Embedding(vocab_size, d_model)
    1:         self.pos_encoder = PositionalEncoding(d_model)
    1:         self.encoder = ModifiedEncoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)
    1:         self.decoder = ModifiedDecoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)
    1:         self.output_layer = nn.Linear(d_model, num_classes)
       
    1:     def forward(self, input_ids, attention_mask=None):
               # BERT's input_ids can directly be used
               src = self.embedding(input_ids) * math.sqrt(d_model)
               src = self.pos_encoder(src)
               memory = self.encoder(src)
               
               output = self.output_layer(memory.mean(dim=1))
               return output
       
       # Model parameters
    1: vocab_size = tokenizer.vocab_size
    1: d_model = 128
    1: ffn_hidden = 512
    1: num_heads = 8
    1: drop_prob = 0.1
    1: num_layers = 3
    1: num_classes = 2
       
    2: model = ModifiedTransformerModel(
    1:     d_model=d_model,
    1:     ffn_hidden=ffn_hidden,
    1:     num_heads=num_heads,
    1:     drop_prob=drop_prob,
    1:     num_layers=num_layers,
    1:     vocab_size=vocab_size,
    1:     num_classes=num_classes
       )
       
       # Training the model
    1: optimizer = optim.Adam(model.parameters())
    1: criterion = nn.CrossEntropyLoss()
    1: epochs = 3
    2: @profile
    2: def train():
           for epoch in range(epochs):
               model.train()
               epoch_loss = 0
               for batch in train_loader:
                   input_ids = batch['input_ids']
                   labels = batch['label']
               
                   optimizer.zero_grad()
                   predictions = model(input_ids)
                   loss = criterion(predictions, labels)
                   loss.backward()
                   optimizer.step()
                   epoch_loss += loss.item()
       
           print(f"Pre-Pruning - Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}")
       
       # Save the model before pruning
    1: torch.save(model.state_dict(), "model_pre_pruning.pth")
    1: print("Model state saved (pre-pruning).")
    1: train()
