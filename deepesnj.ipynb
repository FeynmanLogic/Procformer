{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHPqAfoY0TO0"
      },
      "source": [
        "# Implementation of a KANBoost\n",
        "## Initialisations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "# Define the Echo State Network (ESN) class\n",
        "class EchoStateNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, reservoir_size, output_dim, spectral_radius=0.95):\n",
        "        super(EchoStateNetwork, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.reservoir_size = reservoir_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights for input to reservoir\n",
        "        self.Win = torch.randn(reservoir_size, input_dim) * 0.1\n",
        "\n",
        "        # Initialize reservoir weights\n",
        "        W = torch.randn(reservoir_size, reservoir_size) * 0.1\n",
        "        max_eigenvalue = max(abs(np.linalg.eigvals(W.numpy())))\n",
        "        self.W = nn.Parameter(torch.tensor(W * (spectral_radius / max_eigenvalue), requires_grad=False))\n",
        "\n",
        "        # Readout weights (trainable)\n",
        "        self.Wout = nn.Linear(reservoir_size, output_dim)\n",
        "\n",
        "        # Reservoir state\n",
        "        self.reservoir_state = torch.zeros(reservoir_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Update reservoir state\n",
        "        self.reservoir_state = torch.tanh(\n",
        "            torch.matmul(self.Win, x) + torch.matmul(self.W, self.reservoir_state)\n",
        "        )\n",
        "\n",
        "        # Output\n",
        "        return self.Wout(self.reservoir_state)\n",
        "\n",
        "# Integrate the ESN into the pipeline\n",
        "\n",
        "def train_esn(train_inputs, train_labels, input_dim, output_dim, reservoir_size=100, epochs=100):\n",
        "    model = EchoStateNetwork(input_dim, reservoir_size, output_dim)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for i in range(train_inputs.size(0)):\n",
        "            x = train_inputs[i]\n",
        "            y = train_labels[i]\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage with preprocessed data\n"
      ],
      "metadata": {
        "id": "JkwH_0wP8pRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70GlGWNa4IJ0",
        "outputId": "8e49cbff-b652-487a-e699-b41bd035541b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1n3Plr3bvLI",
        "outputId": "60070fb7-a8d1-4f40-f635-b7afbe8443b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykan\n",
            "  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading pykan-0.2.8-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykan\n",
            "Successfully installed pykan-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pykan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpmg6yIs91Xj"
      },
      "source": [
        "# Sample inputs\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrVEGBzEcUQc",
        "outputId": "9aa3a82e-d83d-4b06-8626-f071ccf1db9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** DEVICE:  cuda  ******\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"****** DEVICE: \",device , \" ******\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RE1svm9cXkX"
      },
      "source": [
        "# KANBoost Preprocessor Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUAWsM-U8ilW"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self, page_size, block_size):\n",
        "        self.page_size = page_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def ensure_48bit_address(self, load_address):\n",
        "        # Ensure the load_address is a 48-bit binary string\n",
        "        return bin(int(load_address, 16))[2:].zfill(48)\n",
        "\n",
        "    def calculate_delta(self, block1, block2):\n",
        "        # Calculate the delta between two blocks (binary subtraction)\n",
        "        return int(block1, 2) - int(block2, 2)\n",
        "\n",
        "    def split_load_address(self, load_address):\n",
        "        binary_address = self.ensure_48bit_address(load_address)\n",
        "\n",
        "        page = binary_address[:self.page_size]  # x Bit (Varies)\n",
        "        block = binary_address[self.page_size:self.page_size + self.block_size]  # 6 Bit Fixed\n",
        "        block_offset = binary_address[self.page_size + self.block_size:]  # Remaining bits\n",
        "\n",
        "        return (page, block, block_offset)\n",
        "\n",
        "\n",
        "    def unsplit_load_address(self, load_address, block_delta):\n",
        "        # Convert the load address (hex) to binary\n",
        "        binary_address = self.ensure_48bit_address(load_address)\n",
        "\n",
        "        # Split the binary address into page, block, and offset\n",
        "        current_page = binary_address[:self.page_size]\n",
        "        current_block = binary_address[self.page_size:self.page_size + self.block_size]\n",
        "        current_block_offset = binary_address[self.page_size + self.block_size:]\n",
        "        adjusted_block_int=int(current_block, 2) + block_delta\n",
        "        if(adjusted_block_int<0):\n",
        "          adjusted_block_int=0\n",
        "        # Adjust the block by adding the block delta\n",
        "        adjusted_block = bin(adjusted_block_int)[2:].zfill(self.block_size)  # Add delta to block and ensure correct bit length\n",
        "        # Reconstruct the binary address\n",
        "        reconstructed_binary_address = current_page + adjusted_block + current_block_offset\n",
        "\n",
        "        # Convert the reconstructed binary address to hexadecimal\n",
        "        reconstructed_load_address = hex(int(reconstructed_binary_address, 2))[2:].lower()  # Remove '0x' prefix and convert to uppercase\n",
        "\n",
        "        return reconstructed_load_address\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        input_features = []\n",
        "        output_labels = []\n",
        "        page_blocks = {}\n",
        "        preprocessed_details = []  # Store detailed information\n",
        "\n",
        "        for i in range(len(data) - 1):\n",
        "            instr_id, cycle_count, load_address, instr_ptr, llc_hit_miss = data[i]\n",
        "            current_page, current_block, current_block_offset = self.split_load_address(load_address)\n",
        "\n",
        "            _, _, next_load_address, _, _ = data[i+1]\n",
        "            next_page, next_block, next_block_offset = self.split_load_address(next_load_address)\n",
        "            # Initialize page_blocks if current_page is not present\n",
        "            if current_page not in page_blocks:\n",
        "                page_blocks[current_page] = ['000001']\n",
        "\n",
        "            # Calculate delta values for the past blocks\n",
        "            delta1 = delta2 = delta3 = delta4 = delta5 = 1 + 64\n",
        "\n",
        "            if len(page_blocks[current_page]) > 1:\n",
        "                delta1 = 64 + self.calculate_delta(page_blocks[current_page][-1], page_blocks[current_page][-2])\n",
        "            if len(page_blocks[current_page]) > 2:\n",
        "                delta2 = 64 + self.calculate_delta(page_blocks[current_page][-2], page_blocks[current_page][-3])\n",
        "            if len(page_blocks[current_page]) > 3:\n",
        "                delta3 = 64 + self.calculate_delta(page_blocks[current_page][-3], page_blocks[current_page][-4])\n",
        "            if len(page_blocks[current_page]) > 4:\n",
        "                delta4 = 64 + self.calculate_delta(page_blocks[current_page][-4], page_blocks[current_page][-5])\n",
        "            if len(page_blocks[current_page]) > 5:\n",
        "                delta5 = 64 + self.calculate_delta(page_blocks[current_page][-4], page_blocks[current_page][-5])\n",
        "\n",
        "            # Calculate delta for the next block (relative to current block)\n",
        "            next_delta = self.calculate_delta(next_block, current_block)\n",
        "\n",
        "            # Append input features\n",
        "            input_features.append((instr_id,load_address, delta1, delta2, delta3, delta4, delta5))\n",
        "\n",
        "            # Convert next_delta to a 128-dimensional one-hot array and append as the output label\n",
        "            output_labels.append(next_delta+64)\n",
        "\n",
        "            # Append the current block to the page's block list\n",
        "            page_blocks[current_page].append(current_block)\n",
        "\n",
        "            # Store the details for inspection without delta1, delta2, delta3, and next_delta\n",
        "            details = {\n",
        "                \"instr_id\": instr_id,\n",
        "                \"page\": current_page,\n",
        "                \"block\": current_block,\n",
        "                \"block_offset\": current_block_offset\n",
        "            }\n",
        "            preprocessed_details.append(details)\n",
        "\n",
        "        return input_features, output_labels, preprocessed_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_Qc2L99qkH"
      },
      "source": [
        "# Read Data from file Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMwDaT0icUJD"
      },
      "outputs": [],
      "source": [
        "# Reading the data from the text file (same as before)\n",
        "def read_data_from_file(filename):\n",
        "    data = []\n",
        "    with open(filename, 'r', newline='', encoding='utf-8') as file:  # Specify newline='' to handle any line endings\n",
        "        for line in file:\n",
        "            line = line.rstrip('\\n')  # Strip only the trailing newline to preserve leading newlines if necessary\n",
        "            if line:\n",
        "                # Split by comma and remove any extra spaces\n",
        "                fields = [x.strip() for x in line.split(',')]\n",
        "                instr_id = int(fields[0])\n",
        "                cycle_count = int(fields[1])\n",
        "                load_address = fields[2]\n",
        "                instr_ptr = fields[3]\n",
        "                llc_hit_miss = int(fields[4])\n",
        "\n",
        "                # Append as a tuple\n",
        "                data.append((instr_id, cycle_count, load_address, instr_ptr, llc_hit_miss))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77OmuA_N-LCL"
      },
      "source": [
        "# Prepare Dataset Function Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeHkhwe_-KsJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def prepare_dataset(input_features, output_labels, batch_size=1, device='cpu'):\n",
        "    \"\"\"\n",
        "    Prepares the dataset for training and testing without train-test split, and preserves instruction IDs.\n",
        "\n",
        "    Args:\n",
        "        input_features (list or np.array): The input features for the dataset.\n",
        "        output_labels (list or np.array): The output labels for the dataset.\n",
        "        batch_size (int): The batch size for data loaders. Default is 1.\n",
        "        device (str): The device to store the tensors ('cpu' or 'cuda'). Default is 'cpu'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the processed dataset with 'input', 'labels', and 'instr_ids'.\n",
        "    \"\"\"\n",
        "    # Assuming input_features is a list of tuples like (instr_id, feature1, feature2, ..., featureN)\n",
        "    instr_ids = [x[0] for x in input_features]  # Extracting the instruction IDs\n",
        "    features = [x[2:] for x in input_features]   # Extracting the actual feature data (excluding instr_id)\n",
        "\n",
        "    # Convert to PyTorch tensors and move to the specified device\n",
        "    data_tensor = torch.tensor(features, dtype=torch.float32, device=device)\n",
        "    target_tensor = torch.tensor(output_labels, dtype=torch.long, device=device)  # Assuming labels are integer values\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    data_loader = DataLoader(TensorDataset(data_tensor, target_tensor),\n",
        "                             batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize tensors for inputs and labels\n",
        "    all_inputs = torch.empty(0, data_tensor.size(1), device=device)  # Assuming data_tensor has N features\n",
        "    all_labels = torch.empty(0, dtype=torch.long, device=device)\n",
        "\n",
        "    # Concatenate all data into a single tensor on the specified device\n",
        "    for data, labels in data_loader:\n",
        "        all_inputs = torch.cat((all_inputs, data.to(device)), dim=0)\n",
        "        all_labels = torch.cat((all_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    # Return the dataset as a dictionary, including instruction IDs\n",
        "    dataset = {\n",
        "        'input': all_inputs,\n",
        "        'label': all_labels,\n",
        "        'instr_ids': instr_ids  # Include the instruction IDs in the output (no need to move to device)\n",
        "    }\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  # ------------------------****** Load Dataset for Training only ******  ------------------------\n",
        "def load_dataset(data,target):\n",
        "    # Convert to PyTorch tensors\n",
        "   #This needs to be torch.float32\n",
        "\n",
        "    # Split dataset into train and test sets\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    target_tensor = torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "    # Split dataset deterministically (e.g., first 80% for training, last 20% for testing)\n",
        "    split_index = int(len(data_tensor) * 0.8)  # 80-20 split\n",
        "\n",
        "    train_data = data_tensor[:split_index]\n",
        "    test_data = data_tensor[split_index:]\n",
        "    train_target = target_tensor[:split_index]\n",
        "    test_target = target_tensor[split_index:]\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_target), batch_size=1, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_target), batch_size=1, shuffle=False)\n",
        "\n",
        "    train_inputs = torch.empty(0, 5, device=device)\n",
        "    train_labels = torch.empty(0, dtype=torch.long,device=device)\n",
        "    test_inputs = torch.empty(0, 5, device=device)\n",
        "    test_labels = torch.empty(0,dtype=torch.long,  device=device)\n",
        "\n",
        "    # Concatenate all data into a single tensor on the specified device\n",
        "    for data, labels in train_loader:\n",
        "        train_inputs = torch.cat((train_inputs, data.to(device)), dim=0)\n",
        "        train_labels = torch.cat((train_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    for data, labels in test_loader:\n",
        "        test_inputs = torch.cat((test_inputs, data.to(device)), dim=0)\n",
        "        test_labels = torch.cat((test_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    dataset = {}\n",
        "    dataset['train_input'] = train_inputs\n",
        "    dataset['test_input'] = test_inputs\n",
        "    dataset['train_label'] = train_labels\n",
        "    dataset['test_label'] = test_labels\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0ard0CJ-nvG"
      },
      "source": [
        "# KANBoost Entry Point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xftcgwkt-twn"
      },
      "outputs": [],
      "source": [
        "filename = '/content/drive/MyDrive/482.sphinx3-s2.trace.gz-hashed_perceptron-no-no-no-trace-lru-1core.txt'\n",
        "data = read_data_from_file(filename)\n",
        "\n",
        "# Initialize the KANBoost Preprocessor\n",
        "page_size = 36\n",
        "block_size = 6\n",
        "\n",
        "preprocessor = Preprocessor(page_size, block_size)\n",
        "\n",
        "# Use the preprocess_data function to process the entire data and capture details\n",
        "input_features, output_labels, preprocessed_details = preprocessor.preprocess_data(data)\n",
        "# dataset=prepare_dataset(input_features=input_features,output_labels=output_labels,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge1fj9EWCbPB",
        "outputId": "d63759f1-dde0-4cee-f678-01aab37defa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, '5cb41dc41440', 65, 65, 65, 65, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "input_features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMaYkdQ2Ecw8"
      },
      "outputs": [],
      "source": [
        "last_5_elements = [t[-5:] for t in input_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t30Ttji5B7cm"
      },
      "outputs": [],
      "source": [
        "dataset=load_dataset(last_5_elements,output_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDBtdgGocUHM",
        "outputId": "34ab6675-c6b6-4e1a-f3fc-147a8b1e8118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: torch.Size([234214, 5])\n",
            "Train target shape: torch.Size([234214])\n",
            "Test data shape: torch.Size([58554, 5])\n",
            "Test target shape: torch.Size([58554])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data shape: {}\".format(dataset['train_input'].shape))\n",
        "print(\"Train target shape: {}\".format(dataset['train_label'].shape))\n",
        "print(\"Test data shape: {}\".format(dataset['test_input'].shape))\n",
        "print(\"Test target shape: {}\".format(dataset['test_label'].shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ERosp1iM17"
      },
      "source": [
        "## Creating and Training the Echo state network\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkjQDBTnNFHw"
      },
      "outputs": [],
      "source": [
        "class EchoStateNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, reservoir_size, output_dim, spectral_radius=0.95):\n",
        "        super(EchoStateNetwork, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.reservoir_size = reservoir_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights for input to reservoir\n",
        "        self.Win = torch.randn(reservoir_size, input_dim) * 0.1\n",
        "\n",
        "        # Initialize reservoir weights\n",
        "        W = torch.randn(reservoir_size, reservoir_size) * 0.1\n",
        "        max_eigenvalue = max(abs(torch.linalg.eigvals(W).real))\n",
        "        self.W = torch.nn.Parameter(W * (spectral_radius / max_eigenvalue), requires_grad=False)\n",
        "\n",
        "        # Readout weights (trainable)\n",
        "        self.Wout = torch.nn.Linear(reservoir_size, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure weights are on the same device as input\n",
        "        self.Win = self.Win.to(x.device)\n",
        "        self.W = self.W.to(x.device)\n",
        "\n",
        "        # Batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Initialize or reset reservoir state for each batch\n",
        "        reservoir_state = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
        "\n",
        "        # Update reservoir state\n",
        "        reservoir_state = torch.tanh(\n",
        "            torch.matmul(x, self.Win.T) + torch.matmul(reservoir_state, self.W.T)\n",
        "        )\n",
        "\n",
        "        # Output\n",
        "        return self.Wout(reservoir_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMWEn4X9F4px",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cc777f-fc30-4869-87f0-ee3a860e97bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared.\n"
          ]
        }
      ],
      "source": [
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache.\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()  # Wait for all streams on a CUDA device to finish\n",
        "    print(\"GPU memory cleared.\")\n",
        "clear_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqbDyOV9HR4I"
      },
      "outputs": [],
      "source": [
        "def Train_KANBoost(dataset, model, optimizer, criterion):\n",
        "    def train_acc():\n",
        "        with torch.no_grad():\n",
        "            predictions = torch.argmax(model(dataset['train_input']), dim=1)\n",
        "            return torch.mean((predictions == dataset['train_label']).float())\n",
        "\n",
        "    def test_acc():\n",
        "        with torch.no_grad():\n",
        "            predictions = torch.argmax(model(dataset['test_input']), dim=1)\n",
        "            return torch.mean((predictions == dataset['test_label']).float())\n",
        "\n",
        "    # Training step\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(dataset['train_input'])\n",
        "    loss = criterion(outputs, dataset['train_label'])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    train_accuracy = train_acc()\n",
        "    test_accuracy = test_acc()\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgvQuRIjIrRv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train_model(dataset, num_epochs=10, input_dim=None, output_dim=None, reservoir_size=100):\n",
        "    # Determine the device (CPU or GPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move input and labels to the appropriate device (not the TensorDataset itself)\n",
        "    train_input = dataset['train_input'].to(device)\n",
        "    train_label = dataset['train_label'].to(device)\n",
        "    test_input = dataset['test_input'].to(device)\n",
        "    test_label = dataset['test_label'].to(device)\n",
        "\n",
        "    # Step 1: Create TensorDataset for train and test datasets\n",
        "    train_dataset = TensorDataset(train_input, train_label)\n",
        "    test_dataset = TensorDataset(test_input, test_label)\n",
        "\n",
        "    # Step 2: Create DataLoader for both train and test datasets\n",
        "    batch_size = 5000\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize ESN model\n",
        "    esn_model = EchoStateNetwork(input_dim, reservoir_size, output_dim).to(device)  # Move model to device\n",
        "    optimizer = torch.optim.Adam(esn_model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Step 3: Iterate through the DataLoader for training\n",
        "    for epoch in range(num_epochs):\n",
        "        for train_batch, test_batch in zip(train_loader, test_loader):\n",
        "            train_inputs, train_targets = train_batch\n",
        "            test_inputs, test_targets = test_batch\n",
        "\n",
        "            # Move batch data to the same device\n",
        "            train_inputs = train_inputs.to(device)\n",
        "            train_targets = train_targets.to(device)\n",
        "            test_inputs = test_inputs.to(device)\n",
        "            test_targets = test_targets.to(device)\n",
        "\n",
        "            partial_dataset = {\n",
        "                \"train_input\": train_inputs,\n",
        "                \"train_label\": train_targets,\n",
        "                \"test_input\": test_inputs,\n",
        "                \"test_label\": test_targets\n",
        "            }\n",
        "            Train_KANBoost(partial_dataset, esn_model, optimizer, criterion)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(esn_model.state_dict(), \"trained_esn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = dataset['train_input'].shape[1]\n",
        "train_model(dataset, num_epochs=10, input_dim=input_dim, output_dim=128, reservoir_size=200)\n",
        "print(\"Model saved to trained_esn_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DtFqSkpM9HY",
        "outputId": "bf678e5d-2074-4110-962a-8c52a3ecfa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.0080, Test Accuracy: 0.0048, Loss: 4.9767\n",
            "Train Accuracy: 0.0048, Test Accuracy: 0.0062, Loss: 4.7657\n",
            "Train Accuracy: 0.2332, Test Accuracy: 0.0672, Loss: 4.5726\n",
            "Train Accuracy: 0.2622, Test Accuracy: 0.1126, Loss: 4.3867\n",
            "Train Accuracy: 0.2624, Test Accuracy: 0.4144, Loss: 4.2046\n",
            "Train Accuracy: 0.2688, Test Accuracy: 0.4166, Loss: 3.9978\n",
            "Train Accuracy: 0.1146, Test Accuracy: 0.4150, Loss: 4.3908\n",
            "Train Accuracy: 0.0936, Test Accuracy: 0.3866, Loss: 3.8117\n",
            "Train Accuracy: 0.2654, Test Accuracy: 0.3294, Loss: 3.6886\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2476, Loss: 3.0661\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2670, Loss: 2.9444\n",
            "Train Accuracy: 0.4132, Test Accuracy: 0.2639, Loss: 2.8844\n",
            "Train Accuracy: 0.2246, Test Accuracy: 0.2684, Loss: 3.6358\n",
            "Train Accuracy: 0.2744, Test Accuracy: 0.1866, Loss: 3.2633\n",
            "Train Accuracy: 0.2710, Test Accuracy: 0.0808, Loss: 3.3004\n",
            "Train Accuracy: 0.2682, Test Accuracy: 0.1194, Loss: 3.3396\n",
            "Train Accuracy: 0.2612, Test Accuracy: 0.4208, Loss: 3.3408\n",
            "Train Accuracy: 0.2692, Test Accuracy: 0.4180, Loss: 3.2269\n",
            "Train Accuracy: 0.1108, Test Accuracy: 0.4154, Loss: 4.4446\n",
            "Train Accuracy: 0.0934, Test Accuracy: 0.3880, Loss: 3.1996\n",
            "Train Accuracy: 0.2628, Test Accuracy: 0.3294, Loss: 3.4687\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2486, Loss: 2.6731\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2642, Loss: 2.6766\n",
            "Train Accuracy: 0.4130, Test Accuracy: 0.2645, Loss: 2.7371\n",
            "Train Accuracy: 0.2264, Test Accuracy: 0.2690, Loss: 3.6760\n",
            "Train Accuracy: 0.2796, Test Accuracy: 0.1832, Loss: 3.2337\n",
            "Train Accuracy: 0.2750, Test Accuracy: 0.0830, Loss: 3.2674\n",
            "Train Accuracy: 0.2724, Test Accuracy: 0.1148, Loss: 3.3165\n",
            "Train Accuracy: 0.2662, Test Accuracy: 0.4164, Loss: 3.2919\n",
            "Train Accuracy: 0.2712, Test Accuracy: 0.4178, Loss: 3.1719\n",
            "Train Accuracy: 0.1112, Test Accuracy: 0.4154, Loss: 4.3860\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3880, Loss: 3.1654\n",
            "Train Accuracy: 0.2658, Test Accuracy: 0.3280, Loss: 3.4043\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2456, Loss: 2.7160\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2644, Loss: 2.7115\n",
            "Train Accuracy: 0.4130, Test Accuracy: 0.2645, Loss: 2.7655\n",
            "Train Accuracy: 0.2278, Test Accuracy: 0.2696, Loss: 3.5763\n",
            "Train Accuracy: 0.2794, Test Accuracy: 0.1858, Loss: 3.1980\n",
            "Train Accuracy: 0.2770, Test Accuracy: 0.0818, Loss: 3.2305\n",
            "Train Accuracy: 0.2716, Test Accuracy: 0.1172, Loss: 3.2829\n",
            "Train Accuracy: 0.2646, Test Accuracy: 0.4208, Loss: 3.2710\n",
            "Train Accuracy: 0.2702, Test Accuracy: 0.4178, Loss: 3.1682\n",
            "Train Accuracy: 0.1116, Test Accuracy: 0.4154, Loss: 4.2526\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3880, Loss: 3.1542\n",
            "Train Accuracy: 0.2632, Test Accuracy: 0.3292, Loss: 3.3679\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2518, Loss: 2.6970\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2658, Loss: 2.6887\n",
            "Train Accuracy: 0.4130, Test Accuracy: 0.2634, Loss: 2.7347\n",
            "Train Accuracy: 0.2298, Test Accuracy: 0.2686, Loss: 3.5724\n",
            "Train Accuracy: 0.2794, Test Accuracy: 0.1858, Loss: 3.1945\n",
            "Train Accuracy: 0.2770, Test Accuracy: 0.0800, Loss: 3.2301\n",
            "Train Accuracy: 0.2718, Test Accuracy: 0.1174, Loss: 3.2789\n",
            "Train Accuracy: 0.2656, Test Accuracy: 0.4208, Loss: 3.2698\n",
            "Train Accuracy: 0.2710, Test Accuracy: 0.4210, Loss: 3.1630\n",
            "Train Accuracy: 0.1116, Test Accuracy: 0.4154, Loss: 4.2654\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3880, Loss: 3.1338\n",
            "Train Accuracy: 0.2624, Test Accuracy: 0.3290, Loss: 3.3728\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2516, Loss: 2.6893\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2656, Loss: 2.6871\n",
            "Train Accuracy: 0.4130, Test Accuracy: 0.2665, Loss: 2.7337\n",
            "Train Accuracy: 0.2292, Test Accuracy: 0.2682, Loss: 3.5596\n",
            "Train Accuracy: 0.2820, Test Accuracy: 0.1854, Loss: 3.1812\n",
            "Train Accuracy: 0.2778, Test Accuracy: 0.0810, Loss: 3.2184\n",
            "Train Accuracy: 0.2734, Test Accuracy: 0.1174, Loss: 3.2653\n",
            "Train Accuracy: 0.2662, Test Accuracy: 0.4208, Loss: 3.2550\n",
            "Train Accuracy: 0.2716, Test Accuracy: 0.4210, Loss: 3.1508\n",
            "Train Accuracy: 0.1118, Test Accuracy: 0.4202, Loss: 4.2588\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3880, Loss: 3.1518\n",
            "Train Accuracy: 0.2626, Test Accuracy: 0.3292, Loss: 3.3685\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2520, Loss: 2.6941\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2658, Loss: 2.6906\n",
            "Train Accuracy: 0.4130, Test Accuracy: 0.2645, Loss: 2.7359\n",
            "Train Accuracy: 0.2308, Test Accuracy: 0.2684, Loss: 3.5515\n",
            "Train Accuracy: 0.2832, Test Accuracy: 0.1856, Loss: 3.1780\n",
            "Train Accuracy: 0.2782, Test Accuracy: 0.0800, Loss: 3.2159\n",
            "Train Accuracy: 0.2728, Test Accuracy: 0.1168, Loss: 3.2642\n",
            "Train Accuracy: 0.2668, Test Accuracy: 0.4208, Loss: 3.2559\n",
            "Train Accuracy: 0.2714, Test Accuracy: 0.4210, Loss: 3.1513\n",
            "Train Accuracy: 0.1118, Test Accuracy: 0.4202, Loss: 4.2551\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3902, Loss: 3.1542\n",
            "Train Accuracy: 0.2626, Test Accuracy: 0.3294, Loss: 3.3635\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2522, Loss: 2.6868\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2672, Loss: 2.6841\n",
            "Train Accuracy: 0.4132, Test Accuracy: 0.2662, Loss: 2.7304\n",
            "Train Accuracy: 0.2312, Test Accuracy: 0.2686, Loss: 3.5532\n",
            "Train Accuracy: 0.2842, Test Accuracy: 0.1852, Loss: 3.1780\n",
            "Train Accuracy: 0.2788, Test Accuracy: 0.0800, Loss: 3.2157\n",
            "Train Accuracy: 0.2742, Test Accuracy: 0.1170, Loss: 3.2647\n",
            "Train Accuracy: 0.2676, Test Accuracy: 0.4208, Loss: 3.2554\n",
            "Train Accuracy: 0.2722, Test Accuracy: 0.4210, Loss: 3.1502\n",
            "Train Accuracy: 0.1118, Test Accuracy: 0.4202, Loss: 4.2508\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3902, Loss: 3.1476\n",
            "Train Accuracy: 0.2628, Test Accuracy: 0.3300, Loss: 3.3625\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2530, Loss: 2.6902\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2676, Loss: 2.6881\n",
            "Train Accuracy: 0.4132, Test Accuracy: 0.2667, Loss: 2.7343\n",
            "Train Accuracy: 0.2326, Test Accuracy: 0.2692, Loss: 3.5487\n",
            "Train Accuracy: 0.2832, Test Accuracy: 0.1852, Loss: 3.1764\n",
            "Train Accuracy: 0.2806, Test Accuracy: 0.0802, Loss: 3.2134\n",
            "Train Accuracy: 0.2752, Test Accuracy: 0.1174, Loss: 3.2627\n",
            "Train Accuracy: 0.2684, Test Accuracy: 0.4208, Loss: 3.2530\n",
            "Train Accuracy: 0.2728, Test Accuracy: 0.4210, Loss: 3.1489\n",
            "Train Accuracy: 0.1118, Test Accuracy: 0.4202, Loss: 4.2452\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3902, Loss: 3.1448\n",
            "Train Accuracy: 0.2628, Test Accuracy: 0.3330, Loss: 3.3608\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2530, Loss: 2.6912\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2686, Loss: 2.6886\n",
            "Train Accuracy: 0.4132, Test Accuracy: 0.2670, Loss: 2.7342\n",
            "Train Accuracy: 0.2338, Test Accuracy: 0.2700, Loss: 3.5467\n",
            "Train Accuracy: 0.2842, Test Accuracy: 0.1858, Loss: 3.1759\n",
            "Train Accuracy: 0.2808, Test Accuracy: 0.0802, Loss: 3.2125\n",
            "Train Accuracy: 0.2758, Test Accuracy: 0.1178, Loss: 3.2623\n",
            "Train Accuracy: 0.2686, Test Accuracy: 0.4208, Loss: 3.2528\n",
            "Train Accuracy: 0.2732, Test Accuracy: 0.4210, Loss: 3.1486\n",
            "Train Accuracy: 0.1118, Test Accuracy: 0.4202, Loss: 4.2416\n",
            "Train Accuracy: 0.0932, Test Accuracy: 0.3902, Loss: 3.1424\n",
            "Train Accuracy: 0.2626, Test Accuracy: 0.3328, Loss: 3.3585\n",
            "Train Accuracy: 0.4302, Test Accuracy: 0.2538, Loss: 2.6908\n",
            "Train Accuracy: 0.4286, Test Accuracy: 0.2680, Loss: 2.6884\n",
            "Train Accuracy: 0.4132, Test Accuracy: 0.2687, Loss: 2.7340\n",
            "Model saved to trained_esn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H_knmGIsKoF"
      },
      "outputs": [],
      "source": [
        "cp -r /content/model /content/drive/MyDrive/kanboost3_24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefetch File Generation\n"
      ],
      "metadata": {
        "id": "JTicu9RB4eKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=prepare_dataset(input_features=input_features,output_labels=output_labels,device=device)"
      ],
      "metadata": {
        "id": "UXkEMpHb_CeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['input'][0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6fkdCSC7DKn",
        "outputId": "fac4d7b8-c4dd-4f59-c75b-40fe34a262c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[65., 65., 65., 65., 65.],\n",
              "        [65., 65., 65., 65., 65.],\n",
              "        [66., 63., 50., 63., 63.],\n",
              "        [65., 65., 65., 40., 40.],\n",
              "        [65., 65., 65., 65., 65.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQfnneEmX7P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim=128\n",
        "reservoir_size=200\n",
        "esn_model = EchoStateNetwork(input_dim, reservoir_size, output_dim)\n",
        "\n",
        "# Load the saved state\n",
        "esn_model.load_state_dict(torch.load(\"trained_esn_model.pth\"))\n",
        "\n",
        "# Move to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "esn_model = esn_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "esn_model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnOM8yCP-nr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5031ea37-863a-404e-a1fd-2f31868b4289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-f4a40385aa6c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  esn_model.load_state_dict(torch.load(\"trained_esn_model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EchoStateNetwork(\n",
              "  (Wout): Linear(in_features=200, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xms9z57Mabn",
        "outputId": "9a39a59e-887e-4b24-e1ca-32b46e652064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7_cache_data.zip  0.7_state\t   drive\tkanboost  sample_data\n",
            "0.7_config.yml\t    0.7_state.zip  history.txt\tmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"model\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSiH8Tj0Mgfl",
        "outputId": "f816c5c7-9c8f-4d6a-f8b6-8572d169e770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  model  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_prefetch_file(path, prefetches):\n",
        "    with open(path, 'w') as f:\n",
        "        for instr_id, pf_addr in prefetches:\n",
        "            print(instr_id, pf_addr, file=f)\n",
        "def prefetch_generation(batch_size=5000):\n",
        "    \"\"\"\n",
        "    Prepare dataset with manual batching.\n",
        "    Returns the average inference time per sample in nanoseconds.\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    # Get the total number of samples\n",
        "    total_samples = len(input_features)\n",
        "    total_inference_time = 0\n",
        "    num_batches = 0\n",
        "    prefetches = []\n",
        "\n",
        "    # Create batches manually by splitting the data\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "        batch_inputs = dataset['input'][i:i + batch_size]\n",
        "        num_batches += 1\n",
        "        batch_size_actual = len(batch_inputs)  # Adjust for the last batch\n",
        "\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "        pred = torch.argmax(esn_model(batch_inputs), dim=1)\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # Accumulate total inference time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # Process predictions\n",
        "        for j in range(len(pred)):\n",
        "            instr_id, load_address, _, _, _, _, _ = input_features[i + j]\n",
        "            load_addr = preprocessor.unsplit_load_address(load_address, int(pred[j].item()) - 64)\n",
        "            prefetches.append((instr_id, load_addr))\n",
        "\n",
        "    # Calculate average inference time per sample in nanoseconds\n",
        "    avg_inference_time_per_sample_ns = (total_inference_time / total_samples) * 1e9 if total_samples > 0 else 0\n",
        "    print(f\"Average Inference Time per Sample: {avg_inference_time_per_sample_ns:.2f} nanoseconds\")\n",
        "\n",
        "    return avg_inference_time_per_sample_ns, prefetches\n"
      ],
      "metadata": {
        "id": "CUCbNVt8-emR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefetches=[]\n",
        "avgtime,prefetches=prefetch_generation()\n",
        "generate_prefetch_file('prefetch_1M_model_v0.5.txt', prefetches)"
      ],
      "metadata": {
        "id": "o3XgGcfj_WN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c852c0f-3a7e-489e-eb5d-04ee08bf13fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time per Sample: 134.50 nanoseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(prefetches),len(input_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPN-DITN_XG5",
        "outputId": "6ad1f910-9b6c-4692-9da5-a5593108ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260492, 260492)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOt7uDH2_fTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}